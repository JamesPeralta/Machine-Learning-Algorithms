{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OneHotEncoding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesPeralta/Machine-Learning-Algorithms/blob/master/Miscellaneous/OneHotEncoding/OneHotEncoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wY5zlLOHeuYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word-level one-hot encoding "
      ]
    },
    {
      "metadata": {
        "id": "i_rzgLtUeqVI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qfHaiG2ue7Pt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f857e30d-2a7e-4e9f-ed82-0468d053343a"
      },
      "cell_type": "code",
      "source": [
        "samples = ['The cat sat on the mat. mat. The The The', 'The dog ate my homework.']\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "      \n",
        "max_length = 10\n",
        "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in enumerate(sample.split()):\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1\n",
        "    \n",
        "print(results)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pdEBDLp7i1BF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Character-level one-hot encoding"
      ]
    },
    {
      "metadata": {
        "id": "hzMq-qeIi0zl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a0657a0a-3745-435a-d49a-37d003bd55c6"
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "print(max(token_index.keys()))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1.\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "[[[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  [1. 1. 1. ... 1. 1. 1.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}