{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Callbacks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesPeralta/Machine-Learning-Algorithms/blob/master/Miscellaneous/Keras%20Callbacks/Keras_Callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UISQy7V4cb6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Keras Callbacks\n",
        "### A callback is an object (a class instance implementing specific methods) that is passed to the model in the call to fit and that is called by the model at various points during training.\n",
        "### It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model."
      ]
    },
    {
      "metadata": {
        "id": "XMzEeCMpC8sC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e7ac91e-1d5f-49e5-de29-9d02c5c4b22d"
      },
      "cell_type": "code",
      "source": [
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vpKkxKNQcgjA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model checkpointing — Saving the current weights of the model at different points during training.\n",
        "## Early stopping — Interrupting training when the validation loss is no longer improving (and of course, saving the best model obtained during training).\n",
        "### Use the EarlyStopping callback to interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs. For instance, this callback allows you to interrupt training as soon as you start overfitting, thus avoiding having to retrain your model for a smaller number of epochs"
      ]
    },
    {
      "metadata": {
        "id": "AVFAcWucdjJX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Callbacks are passed to the model as a list in the fit method\n",
        "callbacks_list = [\n",
        "        keras.callbacks.EarlyStopping( # Interrupts training when improvment stops\n",
        "            monitor='acc', # Monitors the model's validation accuracy\n",
        "            patience=1, # Number of epochs to wait before stopping\n",
        "        ), \n",
        "        keras.callbacks.ModelCheckpoint( # Saves the current weights after every epoch \n",
        "            filepath='my_model.h5', # Path to the destination model file\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,) # won't overwrite the model file unless val_loss has improved\n",
        "]\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc']) # Since we are monitoring accuracy it should be part of the models metrics\n",
        "\n",
        "model.fit(x, y,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=callbacks_list, # Pass in the Callback list\n",
        "          validation_data=(x_val, y_val)) # since we are will also monitor validation_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYaTH0-KdUqZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dynamically adjusting the value of certain parameters during training — Such as the learning rate of the optimizer.\n",
        "### Use this callback to reduce the learning rate when the validation loss has stopped improving. Reducing or increasing the learning rate in case of a loss plateau is is an effective strategy to get out of local minima during training."
      ]
    },
    {
      "metadata": {
        "id": "dPA2uirUdhpX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "callbacks_list = [\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor='val_loss',\n",
        "      factor=0.1,\n",
        "      patience=10,\n",
        "    )\n",
        "]\n",
        "\n",
        "model.fit(x, y,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "97pCWFP4Fr1o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Writing Custom Callbacks\n",
        "### If you need to take a specific action during training that isn’t covered by one of the built-in callbacks, you can write your own callback. Callbacks are implemented by subclassing the class keras.callbacks.Callback.\n",
        "### You can implement any one of these methods:\n",
        "```\n",
        "on_epoch_begin <-- Called at the start of every epoch \n",
        "on_epoch_end   <-- Called at the end of every epoch\n",
        "\n",
        "on_batch_begin <-- Called right before processing each batch\n",
        "on_batch_end   <-- Called right after processing each batch\n",
        "\n",
        "on_train_begin <-- Called at the start of training\n",
        "on_train_end   <-- Called at the end of training\n",
        "```\n",
        "### These methods all are called with a logs argument, which is a dictionary containing information about the previous batch, epoch, or training run: training and validation metrics, etc. The call back also has access to the following attributes\n",
        "*   self.model—The model instance from which the callback is being called\n",
        "*   self.validation_data—The value of what was passed to fit as validation data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "yobQLWzkFsHw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ex) Write a custom callback that saves to disk (as Numpy arrays) the activations of every layer of the model at the end of every epoch, computed on the first sample of the validation set"
      ]
    },
    {
      "metadata": {
        "id": "_wb_o-MlHvyL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ActivationLogger(keras.callbacks.Callback):\n",
        "  \n",
        "  def set_model(self, model):\n",
        "    self.model = model # Called by the parent model before training, to inform the callback of what model will be calling it\n",
        "    layer_outputs = [layer.output for layer in model.layers]\n",
        "    self.activations_model = keras.models.Model(model.input,\n",
        "                                                layer_outputs)\n",
        "    \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if self.validation_data is None:\n",
        "        raise RuntimeError('Requires validation_data.')\n",
        "    validation_sample = self.validation_data[0][0:1]\n",
        "    activations = self.activations_model.predict(validation_sample)\n",
        "    f = open('activations_at_epoch_' + str(epoch) + '.npz', 'w')\n",
        "    np.savez(f, activations)\n",
        "    f.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}